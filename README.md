# NLP for Political Texts (JPM444 Final Exam)

## Project goals

The goal of this project is to use multiple methods from Machine Learning to achieve best
F1 score on a historical data set of Belgian news articles. The assignment requires for two different models to
be created. One to predict whether the issue of the article is domestic or international. And the second one
isto accurately predict whether the article is political or not. 

The full list is as follows:
- Maximise F1 score for both models.
- Create reproducable environment for testing. 
- Write a proper report in academic style. 

## Main methods 

The repository has three main approaches:

- Simple machine learning algorithms using scikit-learn library. 
- Deep learning methods fine-tuning a transformer model using pytorch.  
- A custom approach sending articles in bulk to a big LLM model (Chat-GPT).

The repository is crafted in a way so that each approach could be tested independently. 

## Data
The main dataset is `data/belgium_newspaper_new_filter.csv`. Expected columns:
- `description`: article body (text).
- `headline`: article headline (text).
- `date`: publication date (used to extract year features).
- `political`: label (0/1).
- `domestic`: label (0/1).

## Repository Structure
```text
NLP-for-Political-Texts-JPM444/
├── main.py                           # Entry script (start here)
├── README.md                         # Project overview and usage
├── requirements.txt                  # Python dependencies
├── data                              # Folder with the dataset
├── scripts/
│   ├── api_models.py                 # API-based LLM labeling 
│   ├── baselines_bulk.py             # scikit-learn baselines 
│   ├── bt_transformers.py            # Transformer fine-tuning, zero-shot
│   ├── data_processing.py            # Dataset loading and cleaning
│   ├── ensemble.py                   # Ensemble baselines
│   ├── error_analysis.py             # Error analysis utilities
│   ├── models.py                     # Model registry/constants
│   └── vizuals.py                    # Metrics/plot helpers (legacy)
├── outputs/
│   ├── api_metrics_*.csv
│   ├── baselines_metrics_*.csv
│   ├── ensemble_metrics_*.csv
│   ├── transformers_tune_*.csv
│   ├── bt_transformers/              # Transformer fine-tuning outputs
│   ├── predictions_api/              # API-model predictions/artifacts
│   ├── error_analysis/               # Error analysis artifacts
│   └── plots/                        # Figures/plots
├── notebooks/
│   └── eda_year_labels_tokens.ipynb
├── papers/                           # Background literature (PDFs)
└── report
```

## Setup
Create a virtual environment and install dependencies:

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

If you plan to use the API-based model, create a `.env` file in the repo root:

```
OPENAI_API_KEY=your_key_here
```

With new API rules you would also need a project key

```
OPENAI_PROJECT=your_project_key
```

As for my tests, using Chat-GPT-5.1 and Chat-GPT-5-mini, classifying the whole text costs approximately between 4-8 
US dollars as of 21.01.2026 for one classificatin on 3684 articles (test set size). 


## Usage
All experiments run through `main.py`, which acts as project's orchestrator. 
Boolean flags expect explicit `True`/`False`.

### Baseline models (scikit-learn)
```bash
python3 main.py --baseline_training True --target_var political
```

To limit models:
```bash
python3 main.py \
  --baseline_training True \
  --target_var domestic \
  --baseline_models tfidf_log_reg,count_linear_svm
```

### Ensemble baselines
```bash
python3 main.py --ensemble_training True --target_var political
```

Optional controls:
```bash
python3 main.py \
  --ensemble_training True \
  --target_var domestic \
  --ensemble_num_models 2 \
  --ensemble_models tfidf_log_reg,count_linear_svm
```

The ensemble always uses the best two linear models (logistic regression / linear SVM variants) from the provided candidates and reports both individual and ensemble metrics to `outputs/ensemble_metrics_*.csv`.

### Transformer fine-tuning
```bash
python3 main.py --transformers_tune True --target_var political
```

Optional overrides:
```bash
python3 main.py \
  --transformers_tune True \
  --target_var political \
  --bert_model xlm-roberta-base \
  --epochs_b 3 \
  --bert_batch_size 8
```

### Transformer zero-shot
```bash
python3 main.py --transformers_zero True --target_var domestic
```

### API-based LLM labeling
```bash
python3 main.py --api True --target_var political --api_scope test
```

Optional limits and flushing:
```bash
python3 main.py \
  --api True \
  --target_var domestic \
  --api_max_samples 200 \
  --api_log_every 25 \
  --api_flush_every 25
```

### Plots
Plots are generated by default (`--vizuals True`) and saved to `outputs/plots/`.
You can rerun plotting only by enabling `--vizuals True` with other flags off.

Note: I decided not to pursue good plots, so the features is not at all informative.

## Outputs
- Metrics CSVs: `outputs/baselines_metrics_*.csv`, `outputs/transformers_*_metrics_*.csv`,
  `outputs/api_metrics_*.csv`.
- Training logs: `outputs/transformers_tune_log_*.csv`.
- Plots: `outputs/plots/*.png`.
- Predictions: `predictions_{target}_{model}_seed{seed}.csv` (API runs).

## Reproducibility
- Set `--seed` to control train/test splits and randomness.
- Model parameters and data paths are recorded in the metrics outputs.

## Notes
- API runs require a valid key and internet access.
- Transformer training can be slow without GPU support.
